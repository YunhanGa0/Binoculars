"""
HC3æ•°æ®é›†åŠ è½½å™¨
HC3 Dataset Loader for ChatGPT Detection
"""

import os
import sys
import json
from typing import Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from datasets import Dataset, load_dataset
import pandas as pd


class HC3DatasetLoader:
    """
    HC3æ•°æ®é›†åŠ è½½å™¨
    æ”¯æŒä»HuggingFaceåŠ è½½HC3-Englishå’ŒHC3-Chineseæ•°æ®é›†
    """
    
    def __init__(self, language: str = "english"):
        """
        Args:
            language: "english" æˆ– "chinese"
        """
        self.language = language.lower()
        if self.language == "english":
            self.dataset_name = "Hello-SimpleAI/HC3"
        elif self.language == "chinese":
            self.dataset_name = "Hello-SimpleAI/HC3-Chinese"
        else:
            raise ValueError(f"Unsupported language: {language}. Choose 'english' or 'chinese'")
    
    def load_dataset(self, split: str = "all", local_path: str = None) -> Dataset:
        """
        åŠ è½½HC3æ•°æ®é›†ï¼ˆæ”¯æŒæœ¬åœ°æ–‡ä»¶ï¼‰
        
        Args:
            split: "all" æˆ–ç‰¹å®šå­é›†
            local_path: æœ¬åœ°parquetæ–‡ä»¶è·¯å¾„
        
        Returns:
            Datasetå¯¹è±¡
        """
        # ä¼˜å…ˆä»æœ¬åœ°åŠ è½½
        if local_path and os.path.exists(local_path):
            print(f"Loading from local file: {local_path}")
            if local_path.endswith('.parquet'):
                dataset = Dataset.from_parquet(local_path)
            elif local_path.endswith('.json') or local_path.endswith('.jsonl'):
                dataset = Dataset.from_json(local_path)
            else:
                raise ValueError(f"Unsupported file format: {local_path}")
            print(f"Loaded {len(dataset)} samples from local file")
            return dataset
        
        # å°è¯•ä»æœ¬åœ°ç›®å½•åŠ è½½
        local_dir = os.path.join("datasets", "hc3")
        local_files = [
            os.path.join(local_dir, "all.jsonl"),  # å®Œæ•´æ•°æ®é›†
            os.path.join(local_dir, "hc3_all.jsonl"),
            os.path.join(local_dir, "open_qa.jsonl"),  # å•ä¸ªå­é›†
            os.path.join(local_dir, "finance.jsonl"),
        ]
        
        for local_file in local_files:
            if os.path.exists(local_file):
                print(f"Found local file: {local_file}")
                return self.load_dataset(local_path=local_file)
        
        # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œç»™å‡ºä¸‹è½½æŒ‡å¼•
        print("No local files found.")
        print("\n" + "="*70)
        print("Please download HC3 dataset manually:")
        print("="*70)
        print("\nğŸ“¥ Download Instructions:")
        print("\n1. Visit: https://huggingface.co/datasets/Hello-SimpleAI/HC3/tree/main")
        print("\n2. Download one of these files (recommended: all.jsonl):")
        print("   - all.jsonl (73.7 MB) - Complete dataset â­ RECOMMENDED")
        print("   - open_qa.jsonl (2.91 MB) - Just open QA subset")
        print("   - finance.jsonl (9.89 MB) - Just finance subset")
        print("   - medicine.jsonl (2.68 MB) - Just medicine subset")
        print("   - reddit_eli5.jsonl (55.4 MB) - Just Reddit ELI5 subset")
        print("   - wiki_csai.jsonl (2.2 MB) - Just Wikipedia subset")
        print(f"\n3. Save the file to: {os.path.abspath(local_dir)}/")
        print("\n4. Run this script again")
        print("\n" + "="*70)
        
        raise RuntimeError(
            f"\nâŒ HC3 dataset not found locally.\n"
            f"Please download 'all.jsonl' and save to:\n"
            f"{os.path.abspath(local_dir)}/all.jsonl"
        )
    
    def format_for_binoculars(self, dataset: Dataset, qa_mode: bool = True) -> Dataset:
        """
        å°†HC3æ•°æ®é›†æ ¼å¼åŒ–ä¸ºBinocularså®éªŒæ ¼å¼
        
        Args:
            dataset: HC3æ•°æ®é›†
            qa_mode: æ˜¯å¦ä½¿ç”¨é—®ç­”æ¨¡å¼ï¼ˆQuestion + Answerï¼‰
        
        Returns:
            æ ¼å¼åŒ–åçš„Datasetï¼ŒåŒ…å«human_textå’Œchatgpt_textå­—æ®µ
        """
        formatted_data = []
        
        for item in dataset:
            # HC3æ•°æ®é›†ç»“æ„ï¼š
            # - question: é—®é¢˜
            # - human_answers: äººç±»å›ç­”åˆ—è¡¨
            # - chatgpt_answers: ChatGPTå›ç­”åˆ—è¡¨
            
            question = item.get('question', '')
            human_answers = item.get('human_answers', [])
            chatgpt_answers = item.get('chatgpt_answers', [])
            
            # 1:1é…å¯¹ï¼Œé¿å…é‡å¤
            # å–æœ€å°é•¿åº¦ï¼Œç¡®ä¿æ¯ä¸ªæ ·æœ¬åªé…å¯¹ä¸€æ¬¡
            num_pairs = min(len(human_answers), len(chatgpt_answers))
            
            for i in range(num_pairs):
                human_ans = human_answers[i]
                chatgpt_ans = chatgpt_answers[i]
                
                # å¦‚æœä½¿ç”¨QAæ¨¡å¼ï¼Œå°†é—®é¢˜å’Œç­”æ¡ˆåˆå¹¶
                if qa_mode and question:
                    formatted_data.append({
                        'question': question,
                        'human_text': f"Question: {question}\nAnswer: {human_ans}",
                        'chatgpt_text': f"Question: {question}\nAnswer: {chatgpt_ans}",
                        'human_answer_only': human_ans,
                        'chatgpt_answer_only': chatgpt_ans
                    })
                else:
                    # ä»…ä½¿ç”¨ç­”æ¡ˆéƒ¨åˆ†
                    formatted_data.append({
                        'question': question,
                        'human_text': human_ans,
                        'chatgpt_text': chatgpt_ans,
                        'human_answer_only': human_ans,
                        'chatgpt_answer_only': chatgpt_ans
                    })
        
        return Dataset.from_list(formatted_data)
    
    def create_jsonl_for_experiment(self, 
                                    dataset: Dataset, 
                                    output_path: str,
                                    qa_mode: bool = True,
                                    max_samples: int = None):
        """
        åˆ›å»ºç”¨äºå®éªŒçš„JSONLæ–‡ä»¶ï¼ˆç±»ä¼¼ç°æœ‰çš„cc_news-falcon7.jsonlæ ¼å¼ï¼‰
        
        Args:
            dataset: HC3æ•°æ®é›†
            output_path: è¾“å‡ºè·¯å¾„
            qa_mode: æ˜¯å¦ä½¿ç”¨é—®ç­”æ¨¡å¼
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        """
        formatted_ds = self.format_for_binoculars(dataset, qa_mode=qa_mode)
        
        if max_samples:
            formatted_ds = formatted_ds.select(range(min(max_samples, len(formatted_ds))))
        
        # è½¬æ¢ä¸ºBinocularså®éªŒæ ¼å¼
        # æ¯ä¸ªæ ·æœ¬åŒ…å«humanæ–‡æœ¬å’Œmachineï¼ˆChatGPTï¼‰ç”Ÿæˆçš„æ–‡æœ¬
        output_data = []
        for item in formatted_ds:
            output_data.append({
                "question": item["question"],
                "human_sample": item["human_text"],
                "chatgpt_generated_text": item["chatgpt_text"],
                "human_answer_only": item["human_answer_only"],
                "chatgpt_answer_only": item["chatgpt_answer_only"]
            })
        
        # å†™å…¥JSONLæ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in output_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"Created {len(output_data)} samples in {output_path}")
        return output_path
    
    def get_statistics(self, dataset: Dataset) -> Dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_samples": len(dataset),
            "sample_keys": list(dataset.features.keys()) if dataset else []
        }
        
        if len(dataset) > 0:
            sample = dataset[0]
            if 'human_answers' in sample:
                stats["avg_human_answers"] = sum(len(item.get('human_answers', [])) for item in dataset) / len(dataset)
            if 'chatgpt_answers' in sample:
                stats["avg_chatgpt_answers"] = sum(len(item.get('chatgpt_answers', [])) for item in dataset) / len(dataset)
        
        return stats


def prepare_hc3_for_comparison(language: str = "english", 
                               output_dir: str = "datasets/hc3",
                               qa_mode: bool = True,
                               max_samples: int = None):
    """
    ä¾¿æ·å‡½æ•°ï¼šå‡†å¤‡HC3æ•°æ®é›†ç”¨äºBinocularså®éªŒ
    
    Args:
        language: "english" æˆ– "chinese"
        output_dir: è¾“å‡ºç›®å½•
        qa_mode: æ˜¯å¦ä½¿ç”¨é—®ç­”æ¨¡å¼
        max_samples: æœ€å¤§æ ·æœ¬æ•°
    """
    import os
    
    loader = HC3DatasetLoader(language=language)
    
    # åŠ è½½æ•°æ®é›†
    print(f"Loading HC3-{language.capitalize()} dataset...")
    dataset = loader.load_dataset()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = loader.get_statistics(dataset)
    print(f"Dataset statistics: {stats}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºJSONLæ–‡ä»¶
    mode_suffix = "qa" if qa_mode else "answer_only"
    output_path = os.path.join(output_dir, f"hc3_{language}_{mode_suffix}.jsonl")
    
    loader.create_jsonl_for_experiment(
        dataset=dataset,
        output_path=output_path,
        qa_mode=qa_mode,
        max_samples=max_samples
    )
    
    return output_path


if __name__ == "__main__":
    # å‡†å¤‡HC3è‹±æ–‡æ•°æ®é›†
    print("Preparing HC3 English dataset...")
    en_path = prepare_hc3_for_comparison(
        language="english",
        output_dir="datasets/hc3",
        qa_mode=True,
        max_samples=None  # ä½¿ç”¨å…¨éƒ¨æ•°æ®
    )
    
    print(f"\nDataset prepared: {en_path}")
    print("Ready for evaluation!")
